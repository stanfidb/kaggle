{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "createtfrecord.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMmTo8RbiP2QrFfsTiSQOIz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  print('Running on Colab')\n",
        "else:\n",
        "  print('Not running on Colab')\n",
        "  assert(False)"
      ],
      "metadata": {
        "id": "fYaqtrdY7y7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Kaggle Dataset"
      ],
      "metadata": {
        "id": "4DKtO5t54_ei"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjtXFQ-nSyEj",
        "outputId": "9036987b-842d-4fc5-ca54-7ae3daa48ceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████▋                          | 10 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 58 kB 3.0 MB/s \n",
            "\u001b[?25h  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps --quiet kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "BwvLERD8TTK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c happy-whale-and-dolphin -p /content/happywhale"
      ],
      "metadata": {
        "id": "wjPrWPZwTf54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/happywhale/*.zip -d /content/happywhale/"
      ],
      "metadata": {
        "id": "T0UDnQ1uXeRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/happywhale/*.zip"
      ],
      "metadata": {
        "id": "cjQplWFtXkaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create TF Dataset"
      ],
      "metadata": {
        "id": "eGlkPzjC5Cnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow.data import Dataset"
      ],
      "metadata": {
        "id": "wvDp1pNy5kIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_csv_path = '/content/happywhale/train.csv'\n",
        "train_df = pd.read_csv(train_csv_path)"
      ],
      "metadata": {
        "id": "Ktv7iaEc5guQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_paths = glob.glob(\"/content/happywhale/train_images/*.jpg\")\n",
        "train_img_names = [path.split(os.path.sep)[-1] for path in train_paths]\n",
        "train_species_names = (train_df.set_index('image').loc[train_img_names])['species']\n",
        "train_ids = (train_df.set_index('image').loc[train_img_names])['individual_id']"
      ],
      "metadata": {
        "id": "-c_rLBAiY_tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = Dataset.from_tensor_slices((train_paths, train_img_names, train_species_names, train_ids))\n",
        "N = tf.data.experimental.cardinality(ds).numpy()\n",
        "val_size = int(N * 0.2)\n",
        "train_ds = ds.skip(val_size)\n",
        "val_ds = ds.take(val_size)\n",
        "\n",
        "IMG_HEIGHT = 128\n",
        "IMG_WIDTH = 128\n",
        "def get_family_name(species_name):\n",
        "    parts = tf.strings.split(species_name, '_')\n",
        "    if (parts[-1] == b'whale') or \\\n",
        "       (parts[-1] == b'beluga') or \\\n",
        "       (parts[-1] == b'globis'):\n",
        "       family_name = 'whale'\n",
        "    elif (parts[-1] == b'dolphin') or \\\n",
        "         (parts[-1] == b'dolpin'):\n",
        "         family_name = 'dolphin'\n",
        "    else:\n",
        "        family_name = 'unknown'\n",
        "    return family_name\n",
        "\n",
        "def load_img(path):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
        "    img = tf.cast(img, tf.uint8)\n",
        "    return img\n",
        "\n",
        "def cast_to_float(img):\n",
        "    return tf.cast(img, tf.float32) / 255.\n",
        "\n",
        "ds = ds.map(lambda w,x,y,z: {'image': load_img(w),\n",
        "                             'image_name': x,\n",
        "                             'species_name': y,\n",
        "                             'individual_id': z,\n",
        "                             'family_name': get_family_name(y)}, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds = train_ds.map(lambda w,x,y,z: {'image': load_img(w),\n",
        "                                         'image_name': x,\n",
        "                                         'species_name': y,\n",
        "                                         'individual_id': z,\n",
        "                                         'family_name': get_family_name(y)}, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(lambda w,x,y,z: {'image': load_img(w),\n",
        "                                     'image_name': x,\n",
        "                                     'species_name': y,\n",
        "                                     'individual_id': z,\n",
        "                                     'family_name': get_family_name(y)}, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Cache as 8bit int to save on speed (loading very large images is slow)\n",
        "train_ds = train_ds.cache()\n",
        "val_ds = val_ds.cache()\n",
        "\n",
        "# Then convert to 32bit float\n",
        "train_ds = train_ds.map(lambda data: {'image': cast_to_float(data['image']),\n",
        "                                      'image_name': data['image_name'],\n",
        "                                      'species_name': data['species_name'],\n",
        "                                      'individual_id': data['individual_id'],\n",
        "                                      'family_name': data['family_name']})\n",
        "\n",
        "val_ds = val_ds.map(lambda data: {'image': cast_to_float(data['image']),\n",
        "                                  'image_name': data['image_name'],\n",
        "                                  'species_name': data['species_name'],\n",
        "                                  'individual_id': data['individual_id'],\n",
        "                                  'family_name': data['family_name']})"
      ],
      "metadata": {
        "id": "IjCFrx1kdh_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(3,3, figsize=(10,10)); axs = axs.flatten()\n",
        "for sample,ax in zip(train_ds.take(9),axs):\n",
        "    ax.imshow(sample['image'])\n",
        "    ax.set_title(sample['species_name'].numpy().decode())"
      ],
      "metadata": {
        "id": "IlAU_U0TepC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create TFRecords"
      ],
      "metadata": {
        "id": "5iVYktQc5qNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, random, cv2\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf, re, math\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "mPcRyv6A5rTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _bytes_feature(value):\n",
        "    if isinstance(value, type(tf.constant(0))):\n",
        "        value = value.numpy() # EagerTensor unpackable\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _bytes_array_feature(nonscalar):\n",
        "    if isinstance(nonscalar, type(tf.constant(0))):\n",
        "        nonscalar = nonscalar.numpy()\n",
        "    serialized_nonscalar = tf.io.serialize_tensor(nonscalar)\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[serialized_nonscalar.numpy()]))\n",
        "\n",
        "def serialize_example(image, image_name, species_name, individual_id, family_name):\n",
        "    feature = {\n",
        "        'image': _bytes_array_feature(image),\n",
        "        'image_name': _bytes_feature(image_name),\n",
        "        'species_name': _bytes_feature(species_name),\n",
        "        'individual_id': _bytes_feature(individual_id),\n",
        "        'family_name': _bytes_feature(family_name)\n",
        "    }\n",
        "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "    return example_proto.SerializeToString()\n",
        "\n",
        "def tf_serialize_example(data_dict):\n",
        "    tf_string = tf.py_function(\n",
        "        serialize_example,\n",
        "        (data_dict['image'],\n",
        "         data_dict['image_name'],\n",
        "         data_dict['species_name'],\n",
        "         data_dict['individual_id'],\n",
        "         data_dict['family_name']),\n",
        "         tf.string\n",
        "    )\n",
        "    return tf.reshape(tf_string, ()) # Res is scalar\n",
        "\n",
        "feature_description = {\n",
        "    'image': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "    'image_name': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "    'species_name': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "    'individual_id': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "    'family_name': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "}\n",
        "\n",
        "def _parse_function(example_proto):\n",
        "    # .parse_example if batched\n",
        "    parsed_ex = tf.io.parse_single_example(example_proto, feature_description)\n",
        "    parsed_ex['image'] = tf.io.parse_tensor(parsed_ex['image'], tf.uint8)\n",
        "    return parsed_ex\n",
        "\n",
        "serial_ex = tf_serialize_example(next(iter(ds)))\n",
        "print(serial_ex)\n",
        "print(_parse_function(serial_ex))"
      ],
      "metadata": {
        "id": "eMV-jbjXCbl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "serialized_ds = ds.map(tf_serialize_example)"
      ],
      "metadata": {
        "id": "y6vPTU7-IR2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fname = 'happywhale.tfrecord'\n",
        "writer = tf.data.experimental.TFRecordWriter(fname)\n",
        "writer.write(serialized_ds)"
      ],
      "metadata": {
        "id": "c8U6DBI3Cbol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read TFRecord"
      ],
      "metadata": {
        "id": "G2fZB0yx6AVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fname = ['/content/drive/MyDrive/happywhale.tfrecord']\n",
        "raw_ds = tf.data.TFRecordDataset(fname)\n",
        "raw_ds"
      ],
      "metadata": {
        "id": "qSCcVqc6Cbt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_ds = raw_ds.map(_parse_function)"
      ],
      "metadata": {
        "id": "9WDTYeNiCbwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(parsed_ds))"
      ],
      "metadata": {
        "id": "miGonXF6Cbyq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}